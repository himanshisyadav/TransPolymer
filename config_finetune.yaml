CV_flag: False                                      # whether to use cross-validation
add_vocab_flag: True                             # whether to add supplementary vocab
LLRD_flag: False                                    # whether to use layer-wise lr decay
aug_flag: False                                     # whether to apply data augmentation
aug_special_flag: False                            # whether to augment copolymer SMILES
model_indicator: 'pretrain'                        # whether to use pretrained model
aug_indicator:                                     # number of augmentation per SMILES. If no limitation, assign the indicator with None (leave it blank).

vocab_sup_file: './data/vocab/vocab_sup_PE_I.csv'           # supplementary vocab file path
train_file:  '/project/rcc/hyadav/TransPolymer_3/TransPolymer/data/CE-data/test_CE_exact_all_scaled_nonfusion_no_mol_new.csv'               # train file path
test_file: '/project/rcc/hyadav/TransPolymer_2/data/new-chemprop-data/test_strat_exact_all_scaled_nonfusion_no_mol_new.csv'                     # test file path if cross-validation is not used
model_path: './ckpt/pretrain.pt'                      # pretrain model path
save_path: './ckpt/strat_exact_all_scaled_nonfusion_no_mol_new_huber_5.pt'                     # checkpoint path
best_model_path: './ckpt/strat_exact_all_scaled_nonfusion_no_mol_new_huber_5_best_model.pt'          # best model save path

k: 4                                                # k-fold cross-validation
blocksize: 411                                      # max length of sequences after tokenization
batch_size: 16                                      # batch size
num_epochs: 70                                      # total number of epochs
warmup_ratio: 0.05                                  # warmup ratio
drop_rate: 0.1  #Ionic data                                   # dropout
# drop_rate: 0.01  #CE data                                   # dropout
lr_rate: 0.00005                                    # initial lr for LLRD and pretrained model lr otherwise
lr_rate_reg: 0.00001 #Ionic
# lr_rate_reg: 0.000001 #CE                                # regressor lr if LLRD is not used
# weight_decay: 0.01 #CE
weight_decay: 0.1 #Ionic
hidden_dropout_prob: 0.1                            # hidden layer dropout
attention_probs_dropout_prob: 0.1                  # attention dropout
tolerance: 10                                        # tolerance of no improvement in performance before early stop
num_workers: 8          
